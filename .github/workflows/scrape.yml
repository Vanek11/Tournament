name: Update tournament stats

on:
  workflow_dispatch: {}
  schedule:
    - cron: "23 */6 * * *" # каждые 6 часов
  push:
    paths:
      - "js/data.participants.js"
      - "scrape_lesta.py"
      - ".github/workflows/scrape.yml"

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install deps (pip + Playwright)
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          python -m playwright install --with-deps chromium

      - name: Generate participants.json from js/data.participants.js
        run: |
          python - << 'PY'
          import re, json, pathlib

          src = pathlib.Path("js/data.participants.js").read_text(encoding="utf-8")

          # 1) убрать комментарии // и /* ... */
          src = re.sub(r'//.*', '', src)
          src = re.sub(r'/\*[\s\S]*?\*/', '', src)

          # 2) вытащить массив между первой '[' и соответствующей ']'
          i, j = src.find('['), src.rfind(']')
          if i == -1 or j == -1 or j <= i:
              raise SystemExit("participants array not found")
          arr_txt = src[i:j+1]

          # 3) привести к JSON:
          #    3.1) добавить кавычки вокруг ключей: name: -> "name":
          arr_txt = re.sub(r'([{\s,])([A-Za-z_][A-Za-z0-9_]*)\s*:', r'\1"\2":', arr_txt)
          #    3.2) убрать «висячие» запятые перед ] и }
          arr_txt = re.sub(r',\s*(]|})', r'\1', arr_txt)

          participants = json.loads(arr_txt)

          # 4) добить id и name из URL, если не указаны
          def parse_from_url(url):
              m = re.search(r'/accounts/(\d+)-([^/]+)/?$', url or '')
              if not m: return None, None
              acc = int(m.group(1))
              nick = m.group(2)
              return acc, nick

          for p in participants:
              acc, nick = parse_from_url(p.get("lestaUrl"))
              if p.get("id") in (None, "", 0) and acc:
                  p["id"] = acc
              if not p.get("name") and nick:
                  p["name"] = nick

          # 5) удалить дубликаты по id (последний побеждает)
          uniq = {}
          for p in participants:
              if "id" in p and p["id"]:
                  uniq[p["id"]] = p
          participants = list(uniq.values())

          pathlib.Path("participants.json").write_text(
              json.dumps(participants, ensure_ascii=False, indent=2),
              encoding="utf-8"
          )
          print("participants.json generated with", len(participants), "records")
          PY

      - name: Run scraper
        run: python scrape_lesta.py --input participants.json --out stats --delay 1.5

      - name: Build stats/index.json (aggregate)
        run: |
          python - << 'PY'
          import json, glob, os, pathlib
          data = {}
          for path in glob.glob("stats/*.json"):
              base = os.path.basename(path)
              if base in ("index.json", "all.json"): 
                  continue
              d = json.load(open(path, encoding="utf-8"))
              key = str(d.get("accountId") or os.path.splitext(base)[0])
              data[key] = d
          out = pathlib.Path("stats/index.json")
          out.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")
          print("Wrote", out, "with", len(data), "records")
          PY

      - name: Commit stats
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add stats/*.json
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore: update stats [skip ci]"
            git push
          fi
